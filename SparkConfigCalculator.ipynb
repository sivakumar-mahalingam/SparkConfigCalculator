{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510bcd6c",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e059780d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter no.of Nodes: 10\n"
     ]
    }
   ],
   "source": [
    "total_nodes = int(input(\"Enter no.of Nodes: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b0e7f",
   "metadata": {},
   "source": [
    "## Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be17e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter no.of Cores per Node: 16\n"
     ]
    }
   ],
   "source": [
    "cores_per_node = int(input(\"Enter no.of Cores per Node: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf59c1a",
   "metadata": {},
   "source": [
    "## RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da6b70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter size of RAM (GB) per Node:64\n"
     ]
    }
   ],
   "source": [
    "ram_per_node = int(input(\"Enter size of RAM (GB) per Node:\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a212363",
   "metadata": {},
   "source": [
    "## Deployment Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a27e9dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the deployment mode (Cluster/Client): client\n"
     ]
    }
   ],
   "source": [
    "deployment_mode = input(\"Enter the deployment mode (Cluster/Client): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a7f942",
   "metadata": {},
   "source": [
    "### Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "853c1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold(text):\n",
    "    return \"\\033[1m\" + text + \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c692424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Nodes: 10 (1 Master node + 9 Worker nodes)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Nodes: \" + str(total_nodes) + \" (1 Master node + \" + str(total_nodes-1) + \" Worker nodes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5925ba02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m--executor-cores 5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "executor_cores = 5 # Optimal usage, 5 cores per executor\n",
    "yarn_cores = 1 # Yarn Daemon, 1 core per node\n",
    "print(bold(\"--executor-cores \" + str(executor_cores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbf7b875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Cores per Node: 15\n",
      "Total Cores in Cluster: 150 (15 Cores per Node)\n"
     ]
    }
   ],
   "source": [
    "available_cores_per_node = cores_per_node - yarn_cores\n",
    "print(\"Available Cores per Node: \" + str(available_cores_per_node))\n",
    "\n",
    "total_cores_cluster = available_cores_per_node*total_nodes\n",
    "print(\"Total Cores in Cluster: \" + str(total_cores_cluster) + \" (\" + str(available_cores_per_node) + \" Cores per Node)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654f10b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m--num-executors 30\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "num_executors = int(total_cores_cluster/executor_cores)\n",
    "\n",
    "if deployment_mode.lower() == 'cluster':\n",
    "    # Leaving 1 executor for ApplicationManager/Driver\n",
    "    num_executors = num_executors-1\n",
    "\n",
    "print(bold(\"--num-executors \" + str(num_executors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76191a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Parallelism: 300\n"
     ]
    }
   ],
   "source": [
    "default_parallelism = num_executors*executor_cores*2\n",
    "\n",
    "print(\"Default Parallelism: \" + str(default_parallelism))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca65f556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Executors per Node: 3\n",
      "Total memory (GB) per Executor: 21\n",
      "Heap memory (GB) overhead: 3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "executors_per_node = int(available_cores_per_node/executor_cores)\n",
    "print(\"Number of Executors per Node: \" + str(executors_per_node))\n",
    "\n",
    "total_ram_per_executor = int(ram_per_node/executors_per_node)\n",
    "# Heap memory overhead is 10%\n",
    "heap_memory_overhead = math.ceil(total_ram_per_executor*0.1)\n",
    "\n",
    "print(\"Total memory (GB) per Executor: \" + str(total_ram_per_executor))\n",
    "print(\"Heap memory (GB) overhead: \" + str(heap_memory_overhead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3780804e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m--executor-memory 18G\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "executor_memory = total_ram_per_executor - heap_memory_overhead\n",
    "print(bold(\"--executor-memory \" + str(executor_memory) + \"G\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ed3b4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"spark.driver.cores\": 5,\n",
      "  \"spark.driver.memory\": \"18g\",\n",
      "  \"spark.driver.memoryOverhead\": \"3g\",\n",
      "  \"spark.executor.cores\": 5,\n",
      "  \"spark.executor.memory\": \"18g\",\n",
      "  \"spark.executor.memoryOverhead\": \"3g\",\n",
      "  \"spark.executor.instances\": 30,\n",
      "  \"spark.default.parallelism\": 300,\n",
      "  \"spark.sql.shuffle.partitions\": 300\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "spark_configuration = {\n",
    "            'spark.driver.cores': executor_cores,\n",
    "            'spark.driver.memory': str(executor_memory)+'g',\n",
    "            'spark.driver.memoryOverhead': str(heap_memory_overhead)+'g',\n",
    "            'spark.executor.cores': executor_cores,\n",
    "            'spark.executor.memory': str(executor_memory)+'g',\n",
    "            'spark.executor.memoryOverhead': str(heap_memory_overhead)+'g',\n",
    "            'spark.executor.instances': num_executors,\n",
    "            'spark.default.parallelism': default_parallelism,\n",
    "            'spark.sql.shuffle.partitions': default_parallelism,\n",
    "        }\n",
    "\n",
    "import json\n",
    "print(json.dumps(spark_configuration, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b6cb19",
   "metadata": {},
   "source": [
    "Checked: \n",
    "\n",
    "https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html\n",
    "\n",
    "https://codepen.io/mfeet/pen/eeMJZr\n",
    "\n",
    "https://github.com/KanchiShimono/scopt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907f7bdf",
   "metadata": {},
   "source": [
    "To Check:\n",
    "\n",
    "\n",
    "\n",
    "https://blogs.perficient.com/2020/08/25/key-components-calculations-for-spark-memory-management/\n",
    "\n",
    "https://wganesh.medium.com/spark-cluster-sizing-ed70cd7eec33\n",
    "\n",
    "Must Check after above links:\n",
    "\n",
    "http://spark-configuration.luminousmen.com/\n",
    "\n",
    "https://stackoverflow.com/questions/65822771/how-to-calculate-the-executors-cores-and-memory-based-on-a-given-input-file-siz\n",
    "\n",
    "Least Priority:\n",
    "https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-1/\n",
    "\n",
    "https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e151f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
